---
title: "Basic Features"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Basic Features}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 8,
  out.width = "100%"
)
```

```{r setup}
library(alloscore)
library(dplyr)
library(ggplot2)
library(tibble)
library(purrr)
library(rlang)
```

This vignette runs through some of the new functionality added in this version of `alloscore`.

We start with an exponential data generating process across $N = 10$ targets and use the convenience function `add_pdqr_funs` to construct a log-normal forecaster that tries to match the exponential mean. Only cdfs ("p") and quantile functions ("q") need to be added for `alloscore` to operate but I'll add densities too for plotting.
```{r}
N <- 10
y_mean <- 50
y_gen <- function(Ny=N) rexp(Ny, rate = 1/y_mean)

make_m_lnorm <- function(N) {
tibble(
  target_names = LETTERS[1:N],
	dist = "lnorm",
	sdlog = 1 + rpois(N, 8),
  meanlog = log(y_mean) - .5*sdlog,
) %>% add_pdqr_funs(types = c("p", "d", "q"))
}

m_lnorm <- make_m_lnorm(N)
y <- y_gen()
```

We can now score for an array of Ks in one step, returning a data frame of list columns containing the score, the components of the score, the allocations, and information about the optimization proceedure for each K...
```{r}
Ks <- 60:70
a1 <- alloscore(df = m_lnorm, y = y, K = Ks)
a1$xdf[[2]] # allocations 
a1 %>% select(K, score)
```
or via pipes with the necessary parameters added at each step:
```{r}
a1_allocated <- m_lnorm %>% allocate(K = Ks)
a1_piped <- a1_allocated %>% alloscore(y = y)
a1_piped %>% select(K, score)
```

This works by giving the output of `allocate` an `allocated` class attribute and then calling an `alloscore` method for the `allocated` object.  I'm trying to decide whether it makes sense to have a parallel system for oracles or leave oracle comparison in the jury-rigged state it is in now.

The `xs` column contains data frames for each K recording the iteration history of each allocation. I'll try to wrap this into a plot method soon:

```{r}
# history for K = 69
(iters9 <- a1$xs[[9]])
```


```{r}
iters9 %>% rename(`0` = qs) %>% 
  tidyr::pivot_longer(
    cols = -c(target_names), 
    names_to = "iteration", 
    values_to = "allocation") %>% 
  mutate(
    iteration = as.numeric(iteration)
    ) %>% 
  ggplot(aes(x = iteration, y = allocation, color = target_names)) + 
  geom_line() + geom_point() + theme_classic()
```

The weights for each target and the eneralized piecewise linear loss functions used for each target and the parameters used to construct them are stored in attributes with getter methods `weights` and `gpl`:
```{r}
weights(a1)
gpl(a1)
```
$O$ and $U$ are mostly still just placeholders since I haven't yet adapted `allocate` to accept them, but this should be simple. I keep weights separate from the gpl data since they are only involved in optimization, not scoring.


Also, `alloscore` (and `allocate`) can take the forecast (and gpl) info as individual arguments:
```{r}
K <- c(10, 20, 30)
as_indiv <- alloscore(F = m_lnorm$F, Q = m_lnorm$Q, K = K, y = y)
as_df <- m_lnorm %>% allocate(K = K) %>% alloscore(y = y)

full_join(
  as_indiv %>% select(K, score), 
  as_df %>% select(K, score), 
  by = "K"
)
```

For experiments with large `y` samples, scoring as above gets slow and seems like it could create memory shortages, so I added an `alloscore` method for "slim" data frames that avoids copying many gpl function list-columns
```{r, cache=TRUE}
Ks <- seq(10, 1500, by = 10)
a1_allocated <- m_lnorm %>% allocate(K = Ks)
a1_slim <- make_slim(a1_allocated)
ys <- map(1:50, ~y_gen() %>% set_names(m_lnorm$target_names))
a1_slim_scored <- alloscore(a1_slim, ys)
head(a1_slim_scored)
```

This can then be unnested for plotting:
```{r}
a1_scores <- a1_slim_scored %>% select(-xdf) %>% tidyr::unnest(scores)
head(a1_scores, n = 13)
ytot <- ys %>% map_dbl(sum)

n <- length(ytot)
dens <- density(ytot, n = n)

# Make a data frame with the density estimate
density_data <- data.frame(x = dens$x, y = dens$y * 100000)
p <- a1_scores %>% ggplot() + geom_line(aes(x = K, y = score, group = samp), alpha = .1)
  p + geom_line(data = density_data, aes(x = x, y = y), color = "red") + theme_bw()
```


```{r}
oa <- oracle_allocate(m_lnorm, y = y, K = 10:15)
oa1 <- oracle_allocate(y = y, K = 10:15)
oa1 <- oracle_allocate(y = y, K = 10, g = "log(1+x)")
```

FWIW, the time savings seems like it's around 2/3, but my bet is this will increase as we get into hub analyses...
```{r}
N <- 50
m_lnorm2 <- make_m_lnorm(N)
Ks <- seq(25, 300, length.out = 100)
{
cat("new way: ")
start.time <- Sys.time()
allocate(df = m_lnorm2, K = Ks, eps_K = .0001)
end.time <- Sys.time()
print(end.time-start.time)
}

{
cat("bulldozer way: ")
start.time <- Sys.time()
for (Kind in Ks) {
  allocate(df = m_lnorm2, K = Kind, eps_K = .0001)
}
end.time <- Sys.time()
print(end.time-start.time)
}
```

