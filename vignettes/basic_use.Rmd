---
title: "Basic Features"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Basic Features}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 8,
  out.width = "100%"
)
```

```{r setup}
library(alloscore)
library(tidyverse)
library(rlang)
```

This vignette runs through some of the new functionality added in this version of `alloscore`.

We start with an exponential data generating process across $N = 10$ targets and use the convenience function `add_pdqr_funs` to construct a log-normal forecaster that tries to match the exponential mean. Only cdfs ("p") and quantile functions ("q") need to be added for `alloscore` to operate but I'll add densities too for plotting.
```{r}
N <- 10
y_mean <- 50
y_gen <- function(Ny=N) rexp(Ny, rate = 1/y_mean)

make_m_lnorm <- function(N, sd_param, mean_param) {
tibble(
  target_names = LETTERS[1:N],
	dist = "lnorm",
	sdlog = sd_param,
  meanlog = log(mean_param) - .5*sdlog,
) %>% add_pdqr_funs(types = c("p", "d", "q"))
}

m_lnorm1 <- make_m_lnorm(N, 1, y_mean + 15*(1:N))
m_lnorm2 <- make_m_lnorm(N, (1:N)/(N/2), y_mean) 
y <- y_gen()

ggplot() + 
  map(m_lnorm1$f, ~geom_function(fun = ., n = 1000, color = "red")) + 
  map(m_lnorm2$f, ~geom_function(fun = ., n = 1000, color = "blue")) +
  geom_function(fun = function(x) dexp(x, rate = 1/y_mean), n = 1000, linewidth = 1) +
  lims(x=c(0,100))
```

We can now score for an array of Ks in one step, returning a data frame of list columns containing the score, the components of the score, the allocations, and information about the optimization proceedure for each K...
```{r}
Ks <- seq(200, 400, by = 10)
a1 <- alloscore(df = m_lnorm1, y = y, K = Ks)
a1.10 <- alloscore(df = m_lnorm1, y = y, K = 10, against_oracle = FALSE)
a2 <- alloscore(df = m_lnorm2, y = y, K = Ks)
a1$xdf[[2]] # allocations 
a2$xdf[[2]] # allocations 
a1 %>% select(K, score)
```
or via pipes with the necessary parameters added at each step:
```{r}
a1_allocated <- m_lnorm1 %>% allocate(K = Ks)
a2_allocated <- m_lnorm2 %>% allocate(K = Ks)
a1_piped <- a1_allocated %>% alloscore(y = y)
a1_piped %>% select(K, score)
```

This works by giving the output of `allocate` an `allocated` class attribute and then calling an `alloscore` method for the `allocated` object.  I'm trying to decide whether it makes sense to have a parallel system for oracles or leave oracle comparison in the jury-rigged state it is in now.

The `xs` column contains data frames for each K recording the iteration history of each allocation,
and can be plotted with `plot_iterations`.

```{r}
# history for K = 280
(iters9 <- a1$xs[[9]])
plot_iterations(a1, K_to_plot = 280) 
```

The weights for each target and the generalized piecewise linear loss functions used for each target and the parameters used to construct them are stored in attributes with getter methods `weights` and `gpl`:
```{r}
weights(a1)
gpl(a1)
```
$O$ and $U$ are mostly still just placeholders since I haven't yet adapted `allocate` to accept them, but this should be simple. I keep weights separate from the gpl data since they are only involved in optimization, not scoring.


Also, `alloscore` (and `allocate`) can take the forecast (and gpl) info as individual arguments:
```{r}
K <- c(10, 20, 30)
as_indiv <- alloscore(F = m_lnorm1$F, Q = m_lnorm1$Q, K = K, y = y)
as_df <- m_lnorm1 %>% allocate(K = K) %>% alloscore(y = y)

full_join(
  as_indiv %>% select(K, score), 
  as_df %>% select(K, score), 
  by = "K"
)
```

## Working with multiple models and targets

```{r}
times = 1:3
N = 4 # number of targets
y_series <- tibble(
  origin_time = times,  # targets could be temporal
  ys = map(seq_along(origin_time), ~y_gen()))
m1_series <- tibble(
  origin_time = times,
  model = "m1", 
  forecast = map(origin_time, ~make_m_lnorm(N, 1, y_mean + .*10))) %>% 
  full_join(y_series, by = "origin_time")
m2_series <- m1_series %>% mutate(
  model = "m2",
  forecast = map(times, ~make_m_lnorm(N, (1:N)/(N/2), y_mean + .*10))
  )

a_series <- bind_rows(m1_series, m2_series) %>% mutate(
  alloc = map2(forecast, ys, ~alloscore(df = .x, y = .y, K = c(50,300)))
)

a_series_slim <- a_series %>% 
  select(origin_time, model, alloc) %>% 
  mutate(alloc = map(alloc, slim))
a_series_slim

plot_components(a_series, K = 50, 
                scored_col_name = "alloc", 
                origin_time_col_name = "origin_time", 
                model_col_name = "model")

plot_components(a_series_slim, K = 50, 
                scored_col_name = "alloc", 
                origin_time_col_name = "origin_time", 
                model_col_name = "model")

# won't work
# plot_components(a_series, K = 50)
```



## Monte Carlo stream-lining

For experiments with large `y` samples, scoring as above gets slow and seems like it could create memory shortages, so I added an `alloscore` method for "slim" data frames that avoids copying many gpl function list-columns

```{r}
a1_slim <- alloscore(df = m_lnorm1, y = set_names(y, m_lnorm1$target_names), K = Ks, slim = TRUE)  
```


```{r, cache=TRUE}
Ks <- seq(10, 1500, by = 10)
a1_allocated <- m_lnorm1 %>% allocate(K = Ks)
a1_slim <- slim(a1_allocated)
N = 10 # make y_gen produce 10 target samples again
ys <- map(1:50, ~y_gen() %>% set_names(m_lnorm1$target_names))
a1_slim_scored <- alloscore(a1_slim, ys)
head(a1_slim_scored)
```

This can then be unnested for plotting:
```{r}
a1_scores <- a1_slim_scored %>% select(-xdf) %>% tidyr::unnest(scores)
head(a1_scores, n = 13)
ytot <- ys %>% map_dbl(sum)

n <- length(ytot)
dens <- density(ytot, n = n)

# Make a data frame with the density estimate
density_data <- data.frame(x = dens$x, y = dens$y * 100000)
p <- a1_scores %>% ggplot() + geom_line(aes(x = K, y = score, group = samp), alpha = .1)
  p + geom_line(data = density_data, aes(x = x, y = y), color = "red") + theme_bw()
```

## Questions about the oracle

These allocations are the same but I'm not sure they should be.  If not, the "post-processing" code in `allocate` would need to somehow use `g`.
```{r}
oa <- oracle_allocate(gpl_df = a1_allocated, y = y, K = 10)
oa1 <- oracle_allocate(gpl_df = a1_allocated, y = y, K = 10, g = "log(1+x)")
```

## Efficiency of simultaneous K iterations

FWIW, the time savings seems like it's around 4/5, but my bet is this will increase as we get into hub analyses...
```{r}
N <- 50
m_lnorm2 <- make_m_lnorm(N, (1:N)/(N/2), y_mean) 
Ks <- seq(25, 300, length.out = 100)
{
cat("new way: ")
start.time <- Sys.time()
allocate(df = m_lnorm2, K = Ks, eps_K = .0001)
end.time <- Sys.time()
print(end.time-start.time)
}

{
cat("bulldozer way: ")
start.time <- Sys.time()
for (Kind in Ks) {
  allocate(df = m_lnorm2, K = Kind, eps_K = .0001)
}
end.time <- Sys.time()
print(end.time-start.time)
}
```

